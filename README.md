# ğŸ SwarmGPT

> Parallel LLM swarm intelligence â€” run N models simultaneously across Ollama, OpenAI, and Anthropic, then use Particle Swarm Optimization to converge on the best possible answer.

I was inspired after reading The Art Of Randomness by Ronald T. Kneusel, particually around Swarm based Algorithms and playing around with various Nature Based Swarm Optimization Algorithms. so this is a thought exercise of can we give small inference to mini agents and collectivly swarm to a final answer.

---

## How It Works

SwarmGPT treats each LLM call as a **particle in a swarm**. Every agent gets a unique temperature, fires inference in parallel, and its response is scored by a fitness function. The swarm then collectively evolves toward better answers across multiple iterations â€” inspired by [Particle Swarm Optimization (PSO)](https://en.wikipedia.org/wiki/Particle_swarm_optimization).

```
Iteration 1 â”€â”€â–º 20 agents fire in parallel
                  â†“
             Score each response (fitness function)
                  â†“
             Synthesize top-5 into aggregate answer
                  â†“
             PSO: nudge each agent's temperature
                  toward the best-performing region
                  â†“
Iteration 2 â”€â”€â–º 20 agents fire again (smarter temps)
                  â†“
                 ...
                  â†“
Iteration N â”€â”€â–º Final synthesized answer
```

### The Fitness Function

Each response is scored across four dimensions:

| Dimension | Weight | What it measures |
|---|---|---|
| Keyword coverage | 35% | How well the response addresses the original prompt |
| Length adequacy | 25% | Sweet spot around 150 words â€” not too short, not bloated |
| Coherence | 25% | Unique vs repeated sentences â€” penalizes rambling |
| Diversity bonus | 15% | Rewards agents that surface unique insights vs the pack |

### PSO Temperature Update

Each agent's `temperature` is its **position** in the swarm. After every iteration, velocities are updated using the classic PSO rule:

```
velocity = inertia Ã— velocity
         + cognitive Ã— r1 Ã— (personal_best_score - current_score)
         + social    Ã— r2 Ã— (global_best_temp - current_temp)
```

Agents drift toward temperatures that historically produced high-scoring responses, while maintaining enough diversity to keep exploring.

---

## Providers

| Provider | Concurrency | Notes |
|---|---|---|
| **Ollama** | Unlimited | Local, free, no rate limits |
| **OpenAI** | Semaphore-capped (default: 20) | Safe for Tier 1 (500 RPM) |
| **Anthropic** | Semaphore-capped (default: 15) | Safe for Tier 1 (50 RPM) |

Cloud providers use `threading.Semaphore` so agents queue gracefully â€” you'll never get a 429 from firing 20 simultaneous requests.

---

## Installation

**1. Clone and install dependencies**

```bash
git clone https://github.com/yourname/swarmgpt
cd swarmgpt
pip install requests rich numpy python-dotenv openai anthropic
```

**2. Set up your `.env`**

```bash
cp .env.example .env
```

Then edit `.env` with your keys and preferred models (see [Configuration](#configuration) below).

**3. Make sure Ollama is running** *(if using Ollama)*

```bash
ollama serve
# SwarmGPT will auto-pull the model if it's not already downloaded
```

---

## Quick Start

```bash
# Ollama only (no API keys needed)
python swarmgpt.py --prompt "What Should I have for Lunch?"

# OpenAI, 20 agents, 5 iterations
python swarmgpt.py --prompt "What causes inflation?" --providers openai

# Mixed swarm across all three providers
python swarmgpt.py --prompt "Explain backpropagation" \
  --providers ollama openai anthropic \
  --agents 20 \
  --weights "ollama:10,openai:5,anthropic:5"

# Save output to a file
python swarmgpt.py --prompt "Write a product spec for a todo app" \
  --providers openai --agents 15 --iterations 4 --output result.txt
```

---

## Configuration

All defaults live in `.env`. CLI flags override them at runtime.

```env
# â”€â”€ Ollama â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=tinyllama

# â”€â”€ OpenAI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-4o-mini
OPENAI_MAX_CONCURRENT=20

# â”€â”€ Anthropic â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ANTHROPIC_API_KEY=sk-ant-...
ANTHROPIC_MODEL=claude-haiku-4-5-20251001
ANTHROPIC_MAX_CONCURRENT=15

# â”€â”€ Swarm Defaults â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SWARM_AGENTS=20
SWARM_ITERATIONS=5
SWARM_MAX_TOKENS=300
SWARM_TEMP_MIN=0.3
SWARM_TEMP_MAX=1.2

# â”€â”€ Mixed Swarm Weights â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Proportional allocation across providers
SWARM_PROVIDER_WEIGHTS=ollama:10,openai:5,anthropic:5
```

---

## CLI Reference

```
python swarmgpt.py [OPTIONS]

Required:
  --prompt,   -p    The question or task for the swarm

Providers:
  --providers       Space-separated list: ollama openai anthropic
                    (default: ollama)

Swarm:
  --agents,   -n    Total number of parallel agents (default: 20)
  --iterations, -i  PSO iterations (default: 5)
  --max-tokens      Max tokens per agent response (default: 300)
  --temp-min        Min agent temperature (default: 0.3)
  --temp-max        Max agent temperature (default: 1.2)
  --weights         Provider allocation e.g. "ollama:10,openai:5"

Output:
  --output,   -o    Save final answer to a file
```

---

## Example Output

```
ğŸ SwarmGPT
Providers: ollama | openai  |  Agents: 20  |  Iterations: 5
Explain backpropagation in simple terms

  Agent allocation â†’ ollama: 12 | openai: 8

âŸ³  Iteration 1/5
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Agent Scores â€” Iteration 1                                   â”‚
â”œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ID â”‚ Provider â”‚ Temp â”‚ Score â”‚ Tokens â”‚ Preview             â”‚
â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  3 â”‚ openai   â”‚ 0.71 â”‚ 0.821 â”‚   187  â”‚ Backprop is how ... â”‚
â”‚  1 â”‚ ollama   â”‚ 0.54 â”‚ 0.764 â”‚   203  â”‚ Think of it like... â”‚
â”‚ ...â”‚          â”‚      â”‚       â”‚        â”‚                     â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  âœ“ Iter 1 | Best: 0.821 (openai) | Global best: 0.821 | Tokens: 3842

...

ğŸ†  SwarmGPT â€” Final Synthesized Answer
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ Backpropagation is the process by which a neural network    â•‘
â•‘ learns from its mistakes. After making a prediction, the    â•‘
â•‘ network compares its output to the correct answer and       â•‘
â•‘ calculates the error. It then works backwards through each  â•‘
â•‘ layer â€” adjusting the weights of connections proportionally â•‘
â•‘ to how much each one contributed to the error...            â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```
---

## Project Structure

```
swarmgpt/
â”œâ”€â”€ swarmgpt.py      # Main script all swarm logic
â”œâ”€â”€ .env.example     # Config template copy to .env
â”œâ”€â”€ .env             # Your local config (gitignored)
â””â”€â”€ README.md
```

---

## Roadmap

- [ ] Async I/O (`asyncio` + `aiohttp`) for even lower latency
- [ ] Web UI dashboard showing live swarm convergence
- [ ] Pluggable fitness functions (task-specific scoring)
- [ ] Export full swarm history to JSON
- [ ] Support for local HuggingFace models via `transformers`
- [ ] Multi-prompt tournament mode

---

## License

MIT
